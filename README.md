# Thorough Learning

Research on enhancing language model capabilities through a novel two-phase fine-tuning approach that parallels human learning processes.

## 🎯 Research Overview

Thorough Learning is a two-phase fine-tuning approach that aims to mirror human learning processes:

1. **Memorization Phase**: Autoregressive fine-tuning on chapter texts using LoRA
2. **Problem-Solving Phase**: Reinforcement Learning on examples and exercises

## 🔍 Key Concepts

- Utilizes a strong base model (potentially R1) for effective learning
- Implements custom reasoning prompt format for improved problem-solving
- Combines autoregressive and reinforcement learning approaches

## 📊 Research Components

### Methodology
- **Phase 1**: Autoregressive fine-tuning (LoRA) for text comprehension
- **Phase 2**: RL-based training for example and exercise solving
- Custom prompt format with multiple attempt tracking

### Datasets
- AIME 2024
- MATH 500
- Various knowledge-based and problem-solving benchmarks

### Planned Ablation Studies
1. Baseline comparisons
2. Multiple LoRA configurations
3. Example/exercise ratio impact
4. Activation map analysis

## 🚀 Future Applications

- Hosted LLM service with plug-and-play LoRAs
- Automated textbook format conversion
- Open-source LoRA sharing platform

## 📝 Research Goals

1. Validate the effectiveness of two-phase learning
2. Compare performance across different dataset combinations
3. Analyze activation patterns between memorization and problem-solving tasks

## 🤝 Contributing

This research is conducted openly. Contributions and discussions are welcome!

## ⚖️ License

MIT License - #KnowledgeMustBeCompletelyOpen

## 📚 References

[To be added as research progresses]
